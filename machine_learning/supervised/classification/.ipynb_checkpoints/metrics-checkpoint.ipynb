{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Binary Classification\n",
    "\n",
    " * For **binary** classification, we often speak of a **positive class** and a **negative class**, with the understanding that the positive class is the one we are looking for.\n",
    " * Often, accuracy is not a good measure of predictive performance. Consider this: Let us assume 99% of emails are real and 1% is spam. If we create a model which predicts the emails to be 100% real we would essentially get 99% accurate model, which is in this case would be useless for us since it does not predict any spam emails.\n",
    " \n",
    "#### Let's use Logistic Regression for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "# data where target is number 9\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target == 9\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# init, train, predict\n",
    "logi_reg = LogisticRegression(C=0.1, solver='liblinear').fit(X_train, y_train)\n",
    "y_pred = logi_reg.predict(X_test)\n",
    "print(\"Logistic Regression accuracy: {:.3f}\".format(logi_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s inspect the predictions of LogisticRegression using the confusion_matrix function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[401   2]\n",
      " [  8  39]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "The output of binary classification confusion_matrix is a two-by-two array, where the rows correspond to the true classes and the columns correspond to the predicted classes. Each entry counts how often a sample that belongs to the class corresponding to the row (here, “not nine” and “nine”) was classified as the class corresponding to the column. The following plot illustrates this meaning:\n",
    "\n",
    "<img src=\"img/conf.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Entries on the main diagonal of the confusion matrix correspond to correct classifications, while other entries tell us how many samples of one class got mistakenly classified as another class. \n",
    "\n",
    "If we declare “a nine” the positive class, we can relate the entries of the confusion matrix with the terms false positive and false negative. To complete the picture, we call correctly classified samples belonging to the positive class true positives and correctly classified samples belonging to the negative class true negatives. These terms are usually abbreviated FP, FN, TP, and TN and lead to the following interpretation for the confusion matrix:\n",
    "\n",
    "<img src=\"img/conf2.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices help determining which model classifies better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "We know that accuracy is the number of correct predictions (TP and TN) divided by the number of all samples:\n",
    "$$\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    " * Precision measures how many of the samples predicted as positive are actually positive:\n",
    " \n",
    " $$\\text{Precision} = \\frac{TP}{TP+FP} $$\n",
    " <br>\n",
    " * Used as a performance metric when the goal is to **limit** the number of **false positives**.\n",
    " * Precision is also known as positive predictive value (PPV).\n",
    " \n",
    "### Recall\n",
    "\n",
    " * Measures how many of the positive samples are captured by the positive predictions:\n",
    " \n",
    " $$\\text{Recall} = \\frac{TP}{TP+FN} $$\n",
    " <br>\n",
    " * Used as performance metric when it is important to avoid false negatives.\n",
    " * Other names for recall are sensitivity, hit rate, or true positive rate (TPR)\n",
    " \n",
    "**NB!** In the machine learning community, precision and recall are arguably the most commonly used\n",
    "measures for binary classification, but other communities might use other related metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a trade-off between optimizing recall and optimizing precision. while precision and recall are very important measures, looking at only one of them will not provide you with the full picture. \n",
    "\n",
    "### f-score\n",
    "\n",
    " * One way to summarize them is the **f-score** or f-measure, which is with the harmonic mean of precision and recall:\n",
    " \n",
    " $$ \\text{F} = 2 \\cdot \\frac{\\text{precision}\\cdot \\text{recall}}{\\text{precision}+\\text{recall}}$$\n",
    "<br>\n",
    "Let’s run it on the predictions for the “nine vs. rest” dataset that we computed earlier. Here, we will assume that the “nine” class is the positive class (it is labeled as True while the rest is labeled as False ), so the positive class is the minority class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score logistic regression: 0.886\n"
     ]
    }
   ],
   "source": [
    "print(\"f1 score logistic regression: {:.3f}\".format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * A disadvantage of the f-score, however, is that it is harder to interpret and explain than accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    " * If we want a more comprehensive summary of precision, recall, and f 1 -score, we can use the classification_report convenience function to compute all three at once, and print them in a nice format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not nine       0.98      1.00      0.99       403\n",
      "        nine       0.95      0.83      0.89        47\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       450\n",
      "   macro avg       0.97      0.91      0.94       450\n",
      "weighted avg       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred, target_names=['not nine', 'nine']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
