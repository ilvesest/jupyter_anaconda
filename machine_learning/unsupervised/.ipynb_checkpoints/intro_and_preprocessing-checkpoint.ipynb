{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "* no known output\n",
    "* no teacher to instruct the learning algorithm\n",
    "* the learning algorithm is just shown the input data and asked to extract knowledge from this data\n",
    "* finding __patterns__ in data\n",
    "*  __clustering__ customers by their purchases\n",
    "* compressing data using purchase patterns (__dimension reduction__)\n",
    "\n",
    "### Types of Unsupervised Learning\n",
    "\n",
    "* **Clustering**\n",
    "* **Transformations** of the dataset\n",
    "\n",
    "#### <font color='green'>**Clustering:**</font>\n",
    "* Grouping similar samples together.\n",
    "* Sort of classification.\n",
    "\n",
    "#### <font color='green'>**Unsupervised Transformations:**</font> \n",
    "* Original dataset is transformed into more understandable format for humans/algorithm\n",
    "* Such as **dimensionality reduction**. Extracting most important features.\n",
    "* Common application is to reduce data to 1-2 features for **visualization** purposes.\n",
    "* **Principal Component Analysis (PCA)** - An example is topic extraction on collections of text documents. the task is to find the unknown topics that are talked about in each document, and to learn what topics appear in each document.\n",
    "\n",
    "### Challanges in Supervised Learning\n",
    "\n",
    "* evaluating whether the algorithm learned something useful\n",
    "* no label information, so cannot know what the output should be\n",
    "* often the only way to evaluate the result of an unsupervised algorithm is to inspect it manually\n",
    "\n",
    "### Where applied?\n",
    "\n",
    "* unsupervised algorithms are used often in an exploratory setting, when a data scientist wants to understand the data better, rather than as part of a larger automatic system\n",
    "* preprocessing step for supervised algorithms\n",
    "* Learning a new representation of the data can sometimes improve the accuracy/memory/time of supervised algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Scaling\n",
    "\n",
    "* Scaling methods don’t make use of the supervised information, making them unsupervised.\n",
    "* a common practice is to adjust the features so that the data representation is more suitable for scaling-sensitive algorithms such as SVM-s and Neural Networks.\n",
    "* Often, this is a simple per-feature rescaling and shift of the data.\n",
    "* Four different ways to scale data:\n",
    "\n",
    "![title](img/usl_scaling.png)\n",
    "\n",
    "* **StandardScaler**: ensures each features **mean is 0** and the **variance is 1**, bringing all features to the same magnitude. <font color='red'>**However**</font>, this scaling does not ensure any particular minimum and maximum values for the features. \n",
    "* **RobustScaler**: similar to StandardScaler but uses uses the **median** and **quartiles**, instead of mean and variance. <font color='green'>**Ignores**</font> **outliers**.\n",
    "* **MinMaxScaler**: Shifts data so that all features are exactly between 0 and 1.\n",
    "* **Normalizer**: very different kind of rescaling. each data point such that the feature vector has a Euclidean length of 1. Each datapoint is scaled by the inverse of its length. This normalization is often used when only the direction (or angle) of the data matters, not the length of the feature vector.\n",
    "\n",
    "### Applying Data Transformations\n",
    "\n",
    "we need separate training and test sets to evaluate the supervised model we will build after the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unscaled X_train shape: (426, 30)\n",
      "  Scaled X_train shape: (426, 30)\n",
      "\n",
      "\n",
      "Pre-scaling per-feature maximums:\n",
      "[2.811e+01 3.928e+01 1.885e+02 2.501e+03 1.634e-01 2.867e-01 4.268e-01\n",
      " 2.012e-01 3.040e-01 9.575e-02 2.873e+00 4.885e+00 2.198e+01 5.422e+02\n",
      " 3.113e-02 1.354e-01 3.960e-01 5.279e-02 6.146e-02 2.984e-02 3.604e+01\n",
      " 4.954e+01 2.512e+02 4.254e+03 2.226e-01 9.379e-01 1.170e+00 2.910e-01\n",
      " 5.774e-01 1.486e-01]\n",
      "\n",
      "Per-feature maximums after scaling:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Pre-scaling per-feature minimums:\n",
      "[6.981e+00 9.710e+00 4.379e+01 1.435e+02 5.263e-02 1.938e-02 0.000e+00\n",
      " 0.000e+00 1.060e-01 5.024e-02 1.153e-01 3.602e-01 7.570e-01 6.802e+00\n",
      " 1.713e-03 2.252e-03 0.000e+00 0.000e+00 9.539e-03 8.948e-04 7.930e+00\n",
      " 1.202e+01 5.041e+01 1.852e+02 7.117e-02 2.729e-02 0.000e+00 0.000e+00\n",
      " 1.566e-01 5.521e-02]\n",
      "\n",
      "Per-feature minimums after scaling:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# use cancer dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, \n",
    "    cancer.target, random_state=1)\n",
    "\n",
    "# init scaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# the scaler is only provided with the data ( X_train ) when fit is called, \n",
    "# and y_train is not used\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# to actually scale the training data:\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "print('Unscaled X_train shape: {}'.format(X_train.shape))\n",
    "print('  Scaled X_train shape: {}\\n'.format(X_train_scaled.shape))\n",
    "print('\\nPre-scaling per-feature maximums:\\n{}'.format(X_train.max(axis=0)))\n",
    "print('\\nPer-feature maximums after scaling:\\n{}'.format(X_train_scaled.max(axis=0)))\n",
    "print('\\nPre-scaling per-feature minimums:\\n{}'.format(X_train.min(axis=0)))\n",
    "print('\\nPer-feature minimums after scaling:\\n{}'.format(X_train_scaled.min(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The transformed data has the same shape as the original data—the features are simply shifted and scaled.**\n",
    "\n",
    "To apply the SVM to the scaled data, we also need to transform the test set. This is\n",
    "again done by calling the transform method, this time on X_test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-feature minimums afer scaling:\n",
      "[ 0.0336031   0.0226581   0.03144219  0.01141039  0.14128374  0.04406704\n",
      "  0.          0.          0.1540404  -0.00615249 -0.00137796  0.00594501\n",
      "  0.00430665  0.00079567  0.03919502  0.0112206   0.          0.\n",
      " -0.03191387  0.00664013  0.02660975  0.05810235  0.02031974  0.00943767\n",
      "  0.1094235   0.02637792  0.          0.         -0.00023764 -0.00182032]\n",
      "\n",
      "Per-feature maximums afer scaling:\n",
      "[0.9578778  0.81501522 0.95577362 0.89353128 0.81132075 1.21958701\n",
      " 0.87956888 0.9333996  0.93232323 1.0371347  0.42669616 0.49765736\n",
      " 0.44117231 0.28371044 0.48703131 0.73863671 0.76717172 0.62928585\n",
      " 1.33685792 0.39057253 0.89612238 0.79317697 0.84859804 0.74488793\n",
      " 0.9154725  1.13188961 1.07008547 0.92371134 1.20532319 1.63068851]\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('Per-feature minimums afer scaling:\\n{}'.format(X_test_scaled.min(axis=0)))\n",
    "print('\\nPer-feature maximums afer scaling:\\n{}'.format(X_test_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for the test set, after scaling, the minimum and maximum are not 0 and 1. \n",
    "* Some of the features are even outside the [0;1] range!\n",
    "* The explanation is that the MinMaxScaler (and all the other scalers) always applies exactly the same transformation to the training and the test set. This means the transform method always subtracts the training set minimum and divides by the training set range, which might be different from the minimum and range for the test set.\n",
    "\n",
    "### The effect of Preprocessing on Supervised Learning\n",
    "\n",
    "SVM **without** scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, \n",
    "                                                    random_state=0)\n",
    "\n",
    "svm = SVC(C=100, gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test set accuracy: {:.2f}\".format(svm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM **with** MinMaxScaler scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svm = SVC(C=100, gamma='auto')\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Test set accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
